{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.emb = nn.Line(in_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, max_len, model_dims, device):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.emb = torch.zeros((max_len, model_dims, device=device)\n",
    "        pos = torch.arange(0, max_len, device=device)\n",
    "        pos = pos.double().unsqueeze(dim=1)\n",
    "        i = torch.arange(0, model_dims, step=2, device=device)\n",
    "\n",
    "        self.emb[:, 0::2] = torch.sin(pos / (10000 ** (i/model_dims)))\n",
    "        self.emb[:, 1::2] = torch.sin(pos / (10000 ** (i/model_dims)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, seqlen, model_dim = x.size()\n",
    "        return self.emb[:seqlen, :]\n",
    "\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim,  max_len, model_dims, device, dropprob=0.5):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.token_embedding = TokenEmbedding(in_dim, model_dims)\n",
    "        self.pos_emb =PositionalEmbedding(max_len, model_dims, device)\n",
    "        self.drop = nn.Dropout(dropprob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tkn = self.token_embedding(x)\n",
    "        pos = self.pos_emb(x)\n",
    "        return self.drop(tkn + pos)\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LayerNorm, self).__init__()\n",
    "\n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None, eps=1e-16):\n",
    "        \n",
    "        batch_size, num_head, seq_len, d_tensor = k.size()\n",
    "        k_t = k.transpose(2,3)\n",
    "\n",
    "        score = (q @ k_t) / math.sqrt(d_tensor)\n",
    "        if mask is not None:\n",
    "            score.masked_fill(mask == 0, eps)\n",
    "\n",
    "        score = self.softmax(score)\n",
    "        v = v @ score\n",
    "        return v, score\n",
    "\n",
    "\n",
    "class FeedForwardLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, model_dim, hidden_dim, dropprob=0.0):\n",
    "        super(FeedForwardLayer, self).__init__()\n",
    "        self.lin1 = nn.Linear(model_dim, hidden_dim)\n",
    "        self.drop = nn.Dropout(dropprob)\n",
    "        self.act = nn.GELU()\n",
    "        self.lin2 = nn.Linear(hidden_dim, model_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        return self.lin2(x)\n",
    "\n",
    "class MultiHeadAttn(nn.Module):\n",
    "\n",
    "    def __init__(self, num_head, model_dim):\n",
    "        super(MultiHeadAttn, self).__init__()\n",
    "        self.head = num_head\n",
    "        self.attn = AttentionLayer()\n",
    "        self.Q = nn.Linead(model_dim, model_dim)\n",
    "        self.K = nn.Linead(model_dim, model_dim)\n",
    "        self.V = nn.Linead(model_dim, model_dim)\n",
    "        self.out = nn.Linead(model_dim, model_dim)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        q, k, v = Q(q), K(k), V(v)\n",
    "        q, k, v = self.split(q), self.split(k), self.split(v)\n",
    "        o, score = self.attn(q,k, v)\n",
    "        o = self.concat(o)\n",
    "        o = self.out(o)\n",
    "        return o\n",
    "        \n",
    "\n",
    "    def split(self, x):\n",
    "        batch_size, length, model_dim = x.size()\n",
    "        d_dim = model_dim // self.head\n",
    "        return x.view(batch_size, length, self.head, d_dim).transpose(1, 2)\n",
    "\n",
    "    def concat(self, x):\n",
    "        batch_size, head_num, length, d_dim = x.size()\n",
    "        model_dim = head_num * d_dim\n",
    "        return x.transpose(1,2).contiguous().view(batch, length, model_Dim)\n",
    "\n",
    "\n",
    "class EncodeLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, num_head, model_dim, hidden_dim, dropprob=0.0):\n",
    "        super(EncodeLayer, self).__init__()\n",
    "        self.Attn = MultiHeadAttn(num_head, model_dim)\n",
    "        self.norm1 = LayerNorm(model_dim)\n",
    "        self.drop1 = nn.Dropout2d(dropprob)\n",
    "\n",
    "        self.FF = FeedForwardLayer(model_dim, hidden_dim, dropprob)\n",
    "        self.norm2 = LayerNorm(model_dim)\n",
    "        self.drop2 = nn.Dropout2d(dropprob)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x_ = x\n",
    "        x = self.Attn(q=x, k=x, v=x, mask)\n",
    "        x = self.norm1(x + x_)\n",
    "        x = self.drop1(x)\n",
    "\n",
    "        x_ = x\n",
    "        x = self.FF(x)\n",
    "        x = self.norm2(x + x_)\n",
    "        x = self.drop2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecodeLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, num_head, model_dim, hidden_dim, dropprob=0.0):\n",
    "        super(DecodeLayer, self).__init__()\n",
    "        self.Attn1 = MultiHeadAttn(num_head, model_dim)\n",
    "        self.norm1 = LayerNorm(model_dim)\n",
    "        self.drop1 = nn.Dropout2d(dropprob)\n",
    "\n",
    "        self.Attn2 = MultiHeadAttn(num_head, model_dim)\n",
    "        self.norm2 = LayerNorm(model_dim)\n",
    "        self.drop2 = nn.Dropout2d(dropprob)\n",
    "\n",
    "        self.Attn3 = MultiHeadAttn(num_head, model_dim)\n",
    "        self.norm3 = LayerNorm(model_dim)\n",
    "        self.drop3 = nn.Dropout2d(dropprob)\n",
    "\n",
    "    def forward(self, dec, enc, t_mask, s_t_mask):\n",
    "        dec_ = dec\n",
    "        dec = self.Attn1(q=dec, k=dec, v=dec, t_mask)\n",
    "        dec = delf.norm1(dec + dec_)\n",
    "        dec = self.drop1(dec)\n",
    "\n",
    "        if enc is not None:\n",
    "            dec_ = dec\n",
    "            dec = self.Attn1(q=dec, k=enc, v=enc, s_t_mask)\n",
    "            dec = delf.norm2(dec + dec_)\n",
    "            dec = self.drop2(dec)\n",
    "\n",
    "\n",
    "        dec_ = dec\n",
    "        dec = self.Attn2(q=dec, k=dec, v=dec, t_mask)\n",
    "        dec = delf.norm2(dec + dec_)\n",
    "        dec = self.drop2(dec)\n",
    "\n",
    "        return dec\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(EncodeLayer, self).__init__()\n",
    "\n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DecodeLayer, self).__init__()\n",
    "\n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "    def forward(self):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "743f57b7bc7c41e76c22e315820aa88d3233c2a2ed8f0a7f9f0bd9615bf41eb1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('yolov5': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
