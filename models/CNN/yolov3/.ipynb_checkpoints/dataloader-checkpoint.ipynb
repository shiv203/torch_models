{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "innocent-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "played-alarm",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test.json\", \"r\") as f:\n",
    "    paths = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "beginning-species",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fromfiles(filename, dataset):\n",
    "    \n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    with open(dataset, \"r\") as f:\n",
    "        labels = json.load(f)\n",
    "        \n",
    "    out = []\n",
    "    for k in data[\"objects\"]:\n",
    "        c = labels[data[\"objects\"][k][\"name\"]]\n",
    "        tb = data[\"objects\"][k][\"bbox\"]\n",
    "        tb = [tb[\"xmin\"], tb[\"ymin\"], tb[\"xmax\"], tb[\"ymax\"]]\n",
    "        b = [int(tb[0]), int(tb[1]), int(tb[2]), int(tb[3]), int(c)]\n",
    "        out.append(b)\n",
    "    return torch.from_numpy(np.array(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "running-local",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((367, 500, 3), (1, 5))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgf = list(paths.keys())\n",
    "tmp = imgf[random.randint(0, len(imgf)-1)]\n",
    "box = fromfiles(paths[tmp], \"./dataset_name.json\")\n",
    "img = cv2.imread(tmp)\n",
    "img.shape, box.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "specific-cross",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 29, 102, 280, 351,   6],\n",
       "       [ 52,  14, 266, 370,   0],\n",
       "       [199, 121, 250, 204,  11]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "mysterious-chess",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_xyxy_xywh(boxes):\n",
    "    x1 = boxes[..., 0]\n",
    "    y1 = boxes[..., 1]\n",
    "    x2 = boxes[..., 2]\n",
    "    y2 = boxes[..., 3]\n",
    "    c = boxes[..., 4]\n",
    "    \n",
    "    xc = (x1 + x2)/2\n",
    "    yc = (y1 + y2)/2\n",
    "    w = torch.abs(x2 - x1)\n",
    "    h = torch.abs(y2 - y1)\n",
    "    \n",
    "    xc = xc.reshape(-1, 1)\n",
    "    yc = yc.reshape(-1, 1)\n",
    "    w = w.reshape(-1, 1)\n",
    "    h = h.reshape(-1, 1)\n",
    "    c = c.reshape(-1, 1)\n",
    "    #print(xc.shape, yc.shape, w.shape, h.shape)\n",
    "    \n",
    "    out = torch.cat([xc, yc, w, h, c], dim=-1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "mobile-showcase",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got Tensor)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-c6ba249001da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_xyxy_xywh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got Tensor)"
     ]
    }
   ],
   "source": [
    "b = torch.from_numpy(box)\n",
    "print(b)\n",
    "b1 = convert_xyxy_xywh(b)\n",
    "print(b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "conditional-france",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(boxes, img_shape):\n",
    "    x = boxes[..., 0]\n",
    "    y = boxes[..., 1]\n",
    "    w = boxes[..., 2]\n",
    "    h = boxes[..., 3]\n",
    "    c = boxes[..., 4]\n",
    "    x = x/img_shape[1]\n",
    "    y = y/img_shape[0]\n",
    "    w = w/img_shape[1]\n",
    "    h = h/img_shape[0]\n",
    "    x = x.reshape(-1, 1)\n",
    "    y = y.reshape(-1, 1)\n",
    "    w = w.reshape(-1, 1)\n",
    "    h = h.reshape(-1, 1)\n",
    "    c = c.reshape(-1, 1)\n",
    "    \n",
    "    out = torch.cat([x, y, w, h, c], dim=-1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "wooden-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_w_h(b1, b2):\n",
    "    inter = torch.min(b1[..., 0], b2[..., 0]) * torch.min(b1[..., 1], b2[..., 1])\n",
    "    union = b1[..., 0] * b1[..., 1] + b2[..., 0] * b2[..., 1] - inter\n",
    "    return inter / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "variable-vacation",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.from_numpy(np.array(img.shape))\n",
    "b2 = normalize(b1, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "lined-israeli",
   "metadata": {},
   "outputs": [],
   "source": [
    "stride = [2, 4, 8]\n",
    "anchors =  torch.from_numpy(np.array([(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "short-meeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [torch.zeros((3, s1, s1, 6)) for s1 in stride]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "sublime-mason",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2, 2, 6]), torch.Size([3, 4, 4, 6]), torch.Size([3, 8, 8, 6]))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0].shape, targets[1].shape, targets[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "dietary-rotation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5355, dtype=torch.float64)\n",
      "tensor(0.4852, dtype=torch.float64)\n",
      "tensor(0.1809, dtype=torch.float64)\n",
      "tensor(0.4393, dtype=torch.float64)\n",
      "tensor(0.4262, dtype=torch.float64)\n",
      "tensor(0.1484, dtype=torch.float64)\n",
      "tensor(0.3606, dtype=torch.float64)\n",
      "tensor(0.1265, dtype=torch.float64)\n",
      "tensor(0.0329, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for box in b2:\n",
    "    an = iou_w_h(box[2:4], anchors)\n",
    "    sort_an = an.argsort(descending=True, dim=0)\n",
    "    for an_index in sort_an:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-medicine",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-vietnamese",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "featured-luther",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3090,  0.6172,  0.5020,  0.6785,  6.0000],\n",
       "        [ 0.3180,  0.5232,  0.4280,  0.9700,  0.0000],\n",
       "        [ 0.4490,  0.4428,  0.1020,  0.2262, 11.0000]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-premiere",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read x1, y1, x2, y2, class\n",
    "\n",
    "## convert to centroid height width\n",
    "\n",
    "## for a stride add correct labels\n",
    "\n",
    "## add it for three strides \n",
    "\n",
    "## add collate fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "australian-translation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Yolov3Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, paths, dataset, anchors, strides):\n",
    "        super(Yolov3Dataset, self).__init__()\n",
    "        with open(paths, \"r\") as f:\n",
    "            self.paths = json.load(f)\n",
    "        self.imgf = list(self.paths.keys())\n",
    "        self.dataset = dataset\n",
    "        self.anchors = torch.from_numpy(np.array(anchors))\n",
    "        self.strides = strides\n",
    "        self.num_anchors = self.anchors.shape[0]\n",
    "        self.anchors_per_stride = self.num_anchors // len(self.strides)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgf)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.imgf[idx])\n",
    "        boxes = fromfiles(self.paths[self.imgf[idx]], self.dataset)\n",
    "        boxes = convert_xyxy_xywh(boxes)\n",
    "        boxes = normalize(boxes, img.shape)\n",
    "        \n",
    "        targets = [torch.zeros((self.anchors_per_stride, s, s, 6)) for s in self.strides]\n",
    "        for box in boxes:\n",
    "            an = iou_w_h(box[2:4], self.anchors)\n",
    "            sort_an_idx = an.argsort(descending=True, dim=0)\n",
    "            x,y,w,h,c = box\n",
    "            has_an = [False]*self.anchors_per_stride\n",
    "           \n",
    "            for an_idx in sort_an_idx:\n",
    "                scale_idx = an_idx // self.anchors_per_stride\n",
    "               \n",
    "                anchors_on_scale = an_idx % self.anchors_per_stride\n",
    "                curr_stride = self.strides[scale_idx]\n",
    "                i,j = int(curr_stride * x), int(curr_stride * y)\n",
    "                \n",
    "                anchor_val = targets[scale_idx][anchors_on_scale, i, j, 0]\n",
    "                if not has_an[scale_idx] and not anchor_val:\n",
    "                    anchor_val[anchors_on_scale, i, j, 0] = 1\n",
    "                    x, y = curr_stride * x - i, curr_stride * y - j\n",
    "                    w, h = w * curr_stride, h * curr_stride\n",
    "                    targets[anchors_on_scale, i, j, 1:5] = torch.tensor([x, y, w, h])\n",
    "                    targets[anchors_on_scale, i, j, 5] = c\n",
    "                    has_an[scale_idx] = True\n",
    "                    \n",
    "                elif not anchor_val and an[an_idx]:\n",
    "                    targets[anchors_on_scale, i, j, 0] = -1\n",
    "                    \n",
    "                    \n",
    "        return images, targets\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "forbidden-surgery",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Yolov3Dataset(\"./pascal_paths.json\", \"./dataset_name.json\", anchors, [2,4,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "unauthorized-roads",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = [\n",
    "    (0.28, 0.22), (0.38, 0.48), (0.9, 0.78),\n",
    "    (0.07, 0.15), (0.15, 0.11), (0.14, 0.29),\n",
    "    (0.02, 0.03), (0.04, 0.07), (0.08, 0.06),\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "smaller-principle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-175-3202fecd152c>:31: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  scale_idx = an_idx // self.anchors_per_stride\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-178-87c8f4567a75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-175-3202fecd152c>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_stride\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_stride\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors_on_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0manchor_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0manchors_on_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_an\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscale_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0manchor_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0manchor_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0manchors_on_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "for img, tar in dataset:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-machine",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov5",
   "language": "python",
   "name": "yolov5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
