{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "loving-neighbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "prescription-eagle",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test.json\", \"r\") as f:\n",
    "    paths = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "owned-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fromfiles(filename, dataset):\n",
    "    \n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    with open(dataset, \"r\") as f:\n",
    "        labels = json.load(f)\n",
    "        \n",
    "    out = []\n",
    "    for k in data[\"objects\"]:\n",
    "        c = labels[data[\"objects\"][k][\"name\"]]\n",
    "        tb = data[\"objects\"][k][\"bbox\"]\n",
    "        tb = [tb[\"xmin\"], tb[\"ymin\"], tb[\"xmax\"], tb[\"ymax\"]]\n",
    "        b = [int(tb[0]), int(tb[1]), int(tb[2]), int(tb[3]), int(c)]\n",
    "        out.append(b)\n",
    "    return torch.from_numpy(np.array(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "spare-salon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((367, 500, 3), (1, 5))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgf = list(paths.keys())\n",
    "tmp = imgf[random.randint(0, len(imgf)-1)]\n",
    "box = fromfiles(paths[tmp], \"./dataset_name.json\")\n",
    "img = cv2.imread(tmp)\n",
    "img.shape, box.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "imported-vintage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 29, 102, 280, 351,   6],\n",
       "       [ 52,  14, 266, 370,   0],\n",
       "       [199, 121, 250, 204,  11]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "severe-compound",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_xyxy_xywh(boxes):\n",
    "    x1 = boxes[..., 0]\n",
    "    y1 = boxes[..., 1]\n",
    "    x2 = boxes[..., 2]\n",
    "    y2 = boxes[..., 3]\n",
    "    c = boxes[..., 4]\n",
    "    \n",
    "    xc = (x1 + x2)/2\n",
    "    yc = (y1 + y2)/2\n",
    "    w = torch.abs(x2 - x1)\n",
    "    h = torch.abs(y2 - y1)\n",
    "    \n",
    "    xc = xc.reshape(-1, 1)\n",
    "    yc = yc.reshape(-1, 1)\n",
    "    w = w.reshape(-1, 1)\n",
    "    h = h.reshape(-1, 1)\n",
    "    c = c.reshape(-1, 1)\n",
    "    #print(xc.shape, yc.shape, w.shape, h.shape)\n",
    "    \n",
    "    out = torch.cat([xc, yc, w, h, c], dim=-1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "commercial-female",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got Tensor)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-c6ba249001da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_xyxy_xywh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got Tensor)"
     ]
    }
   ],
   "source": [
    "b = torch.from_numpy(box)\n",
    "print(b)\n",
    "b1 = convert_xyxy_xywh(b)\n",
    "print(b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "valid-powder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(boxes, img_shape):\n",
    "    x = boxes[..., 0]\n",
    "    y = boxes[..., 1]\n",
    "    w = boxes[..., 2]\n",
    "    h = boxes[..., 3]\n",
    "    c = boxes[..., 4]\n",
    "    x = x/img_shape[1]\n",
    "    y = y/img_shape[0]\n",
    "    w = w/img_shape[1]\n",
    "    h = h/img_shape[0]\n",
    "    x = x.reshape(-1, 1)\n",
    "    y = y.reshape(-1, 1)\n",
    "    w = w.reshape(-1, 1)\n",
    "    h = h.reshape(-1, 1)\n",
    "    c = c.reshape(-1, 1)\n",
    "    \n",
    "    out = torch.cat([x, y, w, h, c], dim=-1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "framed-night",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_w_h(b1, b2):\n",
    "    inter = torch.min(b1[..., 0], b2[..., 0]) * torch.min(b1[..., 1], b2[..., 1])\n",
    "    union = b1[..., 0] * b1[..., 1] + b2[..., 0] * b2[..., 1] - inter\n",
    "    return inter / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "royal-bench",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.from_numpy(np.array(img.shape))\n",
    "b2 = normalize(b1, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "proved-disney",
   "metadata": {},
   "outputs": [],
   "source": [
    "stride = [2, 4, 8]\n",
    "anchors =  torch.from_numpy(np.array([(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "instructional-beast",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [torch.zeros((3, s1, s1, 6)) for s1 in stride]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bronze-qualification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2, 2, 6]), torch.Size([3, 4, 4, 6]), torch.Size([3, 8, 8, 6]))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0].shape, targets[1].shape, targets[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "driven-newport",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5355, dtype=torch.float64)\n",
      "tensor(0.4852, dtype=torch.float64)\n",
      "tensor(0.1809, dtype=torch.float64)\n",
      "tensor(0.4393, dtype=torch.float64)\n",
      "tensor(0.4262, dtype=torch.float64)\n",
      "tensor(0.1484, dtype=torch.float64)\n",
      "tensor(0.3606, dtype=torch.float64)\n",
      "tensor(0.1265, dtype=torch.float64)\n",
      "tensor(0.0329, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for box in b2:\n",
    "    an = iou_w_h(box[2:4], anchors)\n",
    "    sort_an = an.argsort(descending=True, dim=0)\n",
    "    for an_index in sort_an:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-holder",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-credits",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "egyptian-navigator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3090,  0.6172,  0.5020,  0.6785,  6.0000],\n",
       "        [ 0.3180,  0.5232,  0.4280,  0.9700,  0.0000],\n",
       "        [ 0.4490,  0.4428,  0.1020,  0.2262, 11.0000]])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-nature",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read x1, y1, x2, y2, class\n",
    "\n",
    "## convert to centroid height width\n",
    "\n",
    "## for a stride add correct labels\n",
    "\n",
    "## add it for three strides \n",
    "\n",
    "## add collate fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "scientific-parker",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Yolov3Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, paths, dataset, anchors, strides):\n",
    "        super(Yolov3Dataset, self).__init__()\n",
    "        with open(paths, \"r\") as f:\n",
    "            self.paths = json.load(f)\n",
    "        self.imgf = list(self.paths.keys())\n",
    "        self.dataset = dataset\n",
    "        self.anchors = torch.from_numpy(np.array(anchors))\n",
    "        self.strides = strides\n",
    "        self.num_anchors = self.anchors.shape[0]\n",
    "        self.anchors_per_stride = self.num_anchors // len(self.strides)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgf)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.imgf[idx])\n",
    "        boxes = fromfiles(self.paths[self.imgf[idx]], self.dataset)\n",
    "        boxes = convert_xyxy_xywh(boxes)\n",
    "        boxes = normalize(boxes, img.shape)\n",
    "        \n",
    "        targets = [torch.zeros((self.anchors_per_stride, s, s, 6)) for s in self.strides]\n",
    "        for box in boxes:\n",
    "            an = iou_w_h(box[2:4], self.anchors)\n",
    "            sort_an_idx = an.argsort(descending=True, dim=0)\n",
    "            x,y,w,h,c = box\n",
    "            has_an = [False]*self.anchors_per_stride\n",
    "           \n",
    "            for an_idx in sort_an_idx:\n",
    "                scale_idx = an_idx // self.anchors_per_stride\n",
    "               \n",
    "                anchors_on_scale = an_idx % self.anchors_per_stride\n",
    "                curr_stride = self.strides[scale_idx]\n",
    "                i,j = int(curr_stride * x), int(curr_stride * y)\n",
    "                \n",
    "                anchor_val = targets[scale_idx][anchors_on_scale, i, j, 0]\n",
    "                if not has_an[scale_idx] and not anchor_val:\n",
    "                    targets[scale_idx][anchors_on_scale, i, j, 0] = 1\n",
    "                    x, y = curr_stride * x - i, curr_stride * y - j\n",
    "                    w, h = w * curr_stride, h * curr_stride\n",
    "                    targets[scale_idx][anchors_on_scale, i, j, 1:5] = torch.tensor([x, y, w, h])\n",
    "                    targets[scale_idx][anchors_on_scale, i, j, 5] = c\n",
    "                    has_an[scale_idx] = True\n",
    "                    \n",
    "                elif not anchor_val and an[an_idx]:\n",
    "                    targets[scale_idx][anchors_on_scale, i, j, 0] = -1\n",
    "                    \n",
    "                    \n",
    "        return img, targets\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "furnished-operation",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Yolov3Dataset(\"./pascal_paths.json\", \"./dataset_name.json\", anchors, [2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "regulation-vitamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = [\n",
    "    (0.28, 0.22), (0.38, 0.48), (0.9, 0.78),\n",
    "    (0.07, 0.15), (0.15, 0.11), (0.14, 0.29),\n",
    "    (0.02, 0.03), (0.04, 0.07), (0.08, 0.06),\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "swedish-emission",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-194-f14ede6983bd>:31: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  scale_idx = an_idx // self.anchors_per_stride\n"
     ]
    }
   ],
   "source": [
    "img, target = dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "adapted-clause",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 334, 3)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "above-bedroom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2, 2, 6]), torch.Size([3, 3, 3, 6]), torch.Size([3, 4, 4, 6]))"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[0].shape, target[1].shape, target[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "bridal-ethiopia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 1.0000,  0.6108,  0.4800,  1.1856,  3.5040,  8.0000]],\n",
       "\n",
       "         [[-1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0000,  0.2006,  0.7280,  0.3892,  1.4480,  0.0000],\n",
       "          [ 1.0000,  0.9731,  0.4040,  1.1078,  1.1920,  8.0000]],\n",
       "\n",
       "         [[ 1.0000,  0.0359,  0.9260,  1.1856,  1.8440,  0.0000],\n",
       "          [ 1.0000,  0.7515,  0.4840,  0.4970,  1.0320,  8.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[-1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 1.0000,  0.7545,  0.0120,  0.4910,  1.8720,  0.0000]]]])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "authentic-welcome",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[-1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[-1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 1.0000,  0.1078,  0.7780,  3.5569,  5.5320,  0.0000]],\n",
       "\n",
       "         [[-1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[-1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 1.0000,  0.2545,  0.4520,  1.4910,  3.0960,  8.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 1.0000,  0.3054,  0.7400,  0.5928,  1.7520,  8.0000],\n",
       "          [ 1.0000,  0.6018,  0.1840,  1.1677,  4.3440,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 1.0000,  0.2635,  0.0360,  1.4731,  5.6160,  0.0000],\n",
       "          [ 1.0000,  0.9192,  0.2120,  3.3234,  3.5760,  8.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "sitting-manhattan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [-1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00]],\n",
       "\n",
       "         [[-1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [-1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [-1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [-1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[-1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [-1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00]],\n",
       "\n",
       "         [[-1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [-1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [-1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [-1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 1.0000e+00,  4.3114e-01,  1.1200e-01,  1.4228e+01,  2.2128e+01,\n",
       "            0.0000e+00]],\n",
       "\n",
       "         [[ 1.0000e+00,  5.3892e-02,  1.4400e-01,  5.8922e+00,  2.2464e+01,\n",
       "            0.0000e+00],\n",
       "          [ 1.0000e+00,  1.7964e-02,  8.0800e-01,  5.9641e+00,  1.2384e+01,\n",
       "            8.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00]],\n",
       "\n",
       "         [[ 1.0000e+00,  4.0719e-01,  7.3600e-01,  4.6707e+00,  1.7376e+01,\n",
       "            0.0000e+00],\n",
       "          [ 1.0000e+00,  4.4311e-01,  9.2000e-01,  4.7425e+00,  1.4016e+01,\n",
       "            8.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00]],\n",
       "\n",
       "         [[ 1.0000e+00,  6.7665e-01,  8.4800e-01,  1.3293e+01,  1.4304e+01,\n",
       "            8.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00]]]])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-orchestra",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-wilderness",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov5",
   "language": "python",
   "name": "yolov5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
